{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc18c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import ipympl\n",
    "\n",
    "\n",
    "#import the ipywidgets module and the IPython display function\n",
    "from ipywidgets import interactive, HBox, Layout, Button, GridspecLayout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import os\n",
    "\n",
    "#!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
    "#!jupyter serverextension enable voila --sys-prefix\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f58b4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8565b8b924824efe86a6416d649091a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h1>Analysis of Poetry using Random Forest Classifier</h1>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "headline = widgets.HTML(value=\"<h1>Analysis of Poetry using Random Forest Classifier</h1>\")\n",
    "headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_1 = widgets.HTML(value=\"<h4>Upload data file in .csv format</h4>\")\n",
    "\n",
    "\n",
    "button_execute = widgets.Button(\n",
    "                description='Analyze',\n",
    "                tooltip='success',\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "\n",
    "#upload btn\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='*.csv',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=False  # True to accept multiple files upload else False\n",
    ")\n",
    "\n",
    "#styles\n",
    "button_execute.style.button_color = 'lime'\n",
    "uploader.style.button_color = 'yellow'\n",
    "\n",
    "#hbox and vbox\n",
    "liner = widgets.VBox([text_1, uploader])\n",
    "page = widgets.HBox([ liner])\n",
    "display(page)\n",
    "#df = pd.read_csv(\"./input/all.csv\")\n",
    "content = '';\n",
    "def on_button_clicked(event):\n",
    "        input_file = list(uploader.value.values())[0]\n",
    "        content = input_file['content']\n",
    "        content = io.StringIO(content.decode('utf-8'))\n",
    "        print(\"Analyzing csv file!\")\n",
    "        \n",
    "        \n",
    "        \n",
    "{# In[20]\n",
    "# In[21]:\n",
    "df = pd.read_csv(\"./input/all.csv\")\n",
    "#df.head()\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "author_list = df['author'].tolist()\n",
    "\n",
    "\n",
    "# author_list[0:5]\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sentences1 = df['content'].tolist()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df['content']\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sentences2 = list()\n",
    "for element in sentences1:\n",
    "    element = element.replace(\"\\n\",\" \")\n",
    "    element = element.replace(\"\\r\",\"\")\n",
    "    element = element.replace(\",\",\"\")\n",
    "    element = element.replace(\".\",\"\")\n",
    "    element = element.replace(\";\",\"\")\n",
    "    element = element.replace(\":\",\"\")\n",
    "    element = element.replace(\"?\",\"\")\n",
    "    element = element.replace(\"!\",\"\")\n",
    "    sentences2.append(element)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sent_list = []\n",
    "for i in range (len(sentences2)):\n",
    "    str1 = sentences2[i].split(\" \")\n",
    "    sent_list.append(str1)\n",
    "#print(sent_list[0])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#print(len(sent_list))\n",
    "#print(len(author_list))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Removing the blank spaces\n",
    "b = list()\n",
    "for mylist in sent_list:\n",
    "    temp = list()\n",
    "    for word in mylist:\n",
    "        if word is not \"\":\n",
    "            temp.append(word)\n",
    "    b.append(temp)\n",
    "sent_list = b\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "Nouns = 0\n",
    "Verbs = 0\n",
    "Adjectives = 0\n",
    "noun_count = []\n",
    "verb_count = []\n",
    "adjective_count = []\n",
    "\n",
    "for i in range(len(sent_list)):\n",
    "    for j in range(len(sent_list[i])):\n",
    "        #print(sent_list[i][j])\n",
    "        word = nlp(sent_list[i][j])\n",
    "        for token in word:\n",
    "            if token.pos_ == 'VERB':\n",
    "                Verbs= Verbs + 1\n",
    "            elif token.pos_ == 'NOUN':\n",
    "                Nouns= Nouns + 1\n",
    "            elif token.pos_ == 'ADJ':\n",
    "                Adjectives= Adjectives + 1\n",
    "    noun_count.append(Nouns)\n",
    "    verb_count.append(Verbs)\n",
    "    adjective_count.append(Adjectives)\n",
    "Nouns = 0\n",
    "Verbs = 0\n",
    "Adjective = 0\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#print(len(noun_count), len(verb_count), len(adjective_count))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "df.loc[:, \"noun_count\"] = noun_count\n",
    "df.loc[:, \"verb_count\"] = verb_count\n",
    "df.loc[:, \"adjective_count\"] = adjective_count\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#splitting the text in to words for analysis purpose\n",
    "parsed = list()\n",
    "for i in range(len(sent_list)):\n",
    "    for j in range(len(sent_list[i])):\n",
    "        if sent_list[i][j] not in parsed:\n",
    "            p1 = sent_list[i][j].split(' ')\n",
    "            parsed.append(p1)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#print(\"Total Nouns in the data set\",df['noun_count'].sum())\n",
    "#print(\"Total Verbs in the data set\",df['verb_count'].sum())\n",
    "#print(\"Total Adjectives in the data set\", df['adjective_count'].sum())\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Number Remover\n",
    "\"\"\"\n",
    "number_remove = []\n",
    "for sublist in sent_list:\n",
    "    mysublist = []\n",
    "    for word in sublist:\n",
    "        myword = \"\"\n",
    "        for alphaneumeric in list(word):\n",
    "            if re.search(\"[0-9]\", alphaneumeric):\n",
    "                pass\n",
    "            else:\n",
    "                myword+=alphaneumeric\n",
    "        mysublist.append(myword)\n",
    "    number_remove.append(mysublist)\n",
    "#number_remove[0]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#len(parsed)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "stopwords = STOP_WORDS\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "s = []\n",
    "\n",
    "for i in range(len(sent_list)):\n",
    "    for str in sent_list[i]:\n",
    "        s_list = [word for word in str.split(\" \") if word not in stopwords]\n",
    "        #print(s_list)\n",
    "        str_ = ' '.join(s_list)   \n",
    "    s.append(str_) \n",
    "        #print(s)\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#creating a list of unique words\n",
    "parsed_stop = []\n",
    "for i in range(len(s)):\n",
    "    for word in s[i].split(' '):\n",
    "        if word not in parsed_stop:\n",
    "            parsed_stop.append(word)\n",
    "#print(parsed_stop)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "filtered_sentence =[] \n",
    "for i in range(len(sent_list)):\n",
    "    for j in range(len(sent_list[i])):\n",
    "        #print(sent_list[i][j])\n",
    "        word = nlp(sent_list[i][j])\n",
    "        for token in word:\n",
    "            token_list = []\n",
    "            #toke.lower\n",
    "            token_list.append(token.lower_)\n",
    "            for word in token_list:\n",
    "                lexeme = nlp.vocab[word]\n",
    "                if (lexeme.is_stop == False) and (lexeme.is_punct == False) and (lexeme.is_oov == True):\n",
    "                     filtered_sentence.append(word)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Create counter\n",
    "counts_no_stopwords = collections.Counter(filtered_sentence)\n",
    "\n",
    "#counts_no_stopwords.most_common(15)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'widget')\n",
    "\n",
    "clean_sentences = pd.DataFrame(counts_no_stopwords.most_common(50),\n",
    "                             columns=['words', 'count'])\n",
    "\n",
    "#original1 gayab\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "comment_words = ' '\n",
    "for i in range(len(sent_list)):\n",
    "    for j in range(len(sent_list[i])):\n",
    "        comment_words = comment_words + sent_list[i][j] + ' '         \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "background_color ='white', \n",
    "stopwords = stopwords, \n",
    "min_font_size = 10).generate(comment_words) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "\n",
    "\n",
    "text_3= widgets.HTML(value=\"<h1>Common Words Found in the Poems without stopwords</h1>\");\n",
    "\n",
    "vbox_text3= widgets.VBox([text_3])\n",
    "\n",
    "page = widgets.HBox([vbox_text3])\n",
    "display(page)\n",
    "\n",
    "\n",
    "# Plot horizontal bar graph\n",
    "clean_sentences.sort_values(by='count').plot.barh(fontsize=10,x='words',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"purple\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#original2 gayab\n",
    "# plot the WordCloud image\n",
    "#lt.figure(figsize = (5, 5), facecolor = None) \n",
    "#lt.imshow(wordcloud) \n",
    "#lt.axis(\"off\") \n",
    "#lt.tight_layout(pad = 0) \n",
    "\n",
    "#lt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "cnt_vectorizer = CountVectorizer()\n",
    "features = cnt_vectorizer.fit_transform(s)\n",
    "features_nd = features.toarray()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "df_subset = pd.DataFrame({\"age\": df['age'], \"type\": df['type'], \"Noun_count\":df['noun_count'], \n",
    "                                  \"Verb_count\":df['verb_count'], \"Adjective_count\":df['adjective_count']})\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "df_subset = pd.get_dummies(df_subset)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "Final = np.column_stack((features_nd,df_subset))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(Final, author_list, train_size=0.75,random_state=1234)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "clf1 = MultinomialNB()\n",
    "clf1.fit(X_train, y_train)\n",
    "y_pred = clf1.predict(X_test)\n",
    "#accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf2 =SVC()\n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred = clf2.predict(X_test)\n",
    "#accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "#accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df['type'].unique()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df['author'].unique()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df['age'].unique()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "df_modern = df[df['age'] == 'Modern']\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df_modern.head(10)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df_modern['author'].unique()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "modern_poetry = df_modern[['author', 'poem name']].drop_duplicates()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#modern_poetry.head(10)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "authors = df['author'].unique()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#authors\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "age = df['age'].unique()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "genre = df['type'].unique()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df['age'].value_counts()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#df['type'].value_counts()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "G =nx.Graph()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "G.add_nodes_from(age)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "G.add_nodes_from(genre)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "G.add_nodes_from(authors)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#G.nodes()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "myEdges = [tuple(element) for element in df[['author','type']].values]\n",
    "myEdges2 = [tuple(element) for element in df[['author', 'age']].values]\n",
    "myEdges3 = [tuple(element) for element in df[['age', 'type']].values]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "G.add_edges_from(myEdges)\n",
    "G.add_edges_from(myEdges2)\n",
    "G.add_edges_from(myEdges3)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#myEdges\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "text_2 = widgets.HTML(value=\"<h1>Network Graph</h1>\")\n",
    "\n",
    "vbox_text2 = widgets.VBox([text_2])\n",
    "\n",
    "page = widgets.HBox([vbox_text2])\n",
    "display(page)\n",
    "\n",
    "#%matplotlib inline\n",
    "#fig = plt.figure(3, figsize=(20, 10))\n",
    "fig = plt.figure(3, figsize=(12, 8))\n",
    "nx.draw_spring(G, with_labels=True, font_weight='normal',font_size=12,node_size=100)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#nx.density(G)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#nx.number_of_nodes(G)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#nx.number_of_edges(G)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#nx.closeness_centrality(G)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#nx.betweenness_centrality(G)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#nx.degree_centrality(G)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#cross_tab = pd.crosstab(df['age'],df['type'],margins=True)\n",
    "#cross_tab = pd.crosstab(df['type'],df['age'],margins=True)\n",
    "#cross_tab.iloc[:-1,:-1].plot(kind='barh',stacked=True, color=['red','blue', 'yellow'], grid=False,fontsize=10,figsize=(15,6))\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#print(cross_tab)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#cross_tab2 = pd.crosstab(df['author'],df['type'],margins=True)\n",
    "#print(cross_tab2)\n",
    "\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, squeeze='False')\n",
    "\n",
    "#df1.plot(ax=axes[0,0])\n",
    "#cross_tab.iloc[:-1,:-1].plot(ax=axes[0], kind='bar',stacked=True, color=['red','blue', 'yellow'], grid=False,fontsize=10,figsize=(5,6))\n",
    "#df2.plot(ax=axes[0,1])\n",
    "#cross_tab2.iloc[:-1,:-1].plot(ax=axes[1], kind='bar',stacked=True, color=['red','turquoise', 'green'], grid=False,fontsize=8,figsize=(12,5))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "text_1 = widgets.HTML(value=\"<h1>Types of Genres</h1>\")\n",
    "\n",
    "vbox_text1 = widgets.VBox([text_1])\n",
    "\n",
    "page = widgets.HBox([vbox_text1])\n",
    "display(page)\n",
    "cross_tab = pd.crosstab(df['age'],df['type'],margins=True)\n",
    "\n",
    "cross_tab.iloc[:-1,:-1].plot(kind='barh',stacked=True, color=['red','blue', 'yellow'], grid=False,fontsize=10,figsize=(12,5))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "cross_tab2 = pd.crosstab(df['author'],df['type'],margins=True)\n",
    "#print(cross_tab2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "text_0 = widgets.HTML(value=\"<h1>Analysis of Poetry</h1>\")\n",
    "\n",
    "vbox_text = widgets.VBox([text_0])\n",
    "\n",
    "page = widgets.HBox([vbox_text])\n",
    "display(page)\n",
    "\n",
    "cross_tab2.iloc[:-1,:-1].plot(kind='bar',stacked=True, color=['red','turquoise', 'green'], grid=False,fontsize=8,figsize=(12,10))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "button_execute.on_click(on_button_clicked)\n",
    "button_execute\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
